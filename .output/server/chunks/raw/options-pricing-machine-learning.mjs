// ROLLUP_NO_REPLACE 
 const optionsPricingMachineLearning = "{\"parsed\":{\"_path\":\"/articles/options-pricing-machine-learning\",\"_dir\":\"articles\",\"_draft\":false,\"_partial\":false,\"_locale\":\"\",\"title\":\"Predicting Option Call Pricing Using Machine Learning\",\"description\":\"xyz\",\"published\":\"2024/05/14\",\"slug\":\"option-pricing-machine-learning\",\"body\":{\"type\":\"root\",\"children\":[{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"authors\"},\"children\":[{\"type\":\"text\",\"value\":\"Authors:\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"a\",\"props\":{\"href\":\"https://www.linkedin.com/in/murtadhamr/\",\"rel\":[\"nofollow\"]},\"children\":[{\"type\":\"text\",\"value\":\"Muhammad Murtadha Ramadhan\"}]},{\"type\":\"text\",\"value\":\", \"},{\"type\":\"element\",\"tag\":\"a\",\"props\":{\"href\":\"https://www.linkedin.com/in/amrushabuddhiraju/\",\"rel\":[\"nofollow\"]},\"children\":[{\"type\":\"text\",\"value\":\"Amrusha Buddhiraju\"}]},{\"type\":\"text\",\"value\":\", \"},{\"type\":\"element\",\"tag\":\"a\",\"props\":{\"href\":\"https://www.linkedin.com/in/chinmaysashittal/\",\"rel\":[\"nofollow\"]},\"children\":[{\"type\":\"text\",\"value\":\"Chinmay Atul Sashittal\"}]},{\"type\":\"text\",\"value\":\", \"},{\"type\":\"element\",\"tag\":\"a\",\"props\":{\"href\":\"https://www.linkedin.com/in/evanbenham/\",\"rel\":[\"nofollow\"]},\"children\":[{\"type\":\"text\",\"value\":\"Evan Benham\"}]},{\"type\":\"text\",\"value\":\", \"},{\"type\":\"element\",\"tag\":\"a\",\"props\":{\"href\":\"https://www.linkedin.com/in/muskanagg/\",\"rel\":[\"nofollow\"]},\"children\":[{\"type\":\"text\",\"value\":\"Muskan Aggarwal\"}]},{\"type\":\"text\",\"value\":\", \"},{\"type\":\"element\",\"tag\":\"a\",\"props\":{\"href\":\"https://www.linkedin.com/in/vineetha-jinan/\",\"rel\":[\"nofollow\"]},\"children\":[{\"type\":\"text\",\"value\":\"Vineetha Jinan\"}]}]},{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"background\"},\"children\":[{\"type\":\"text\",\"value\":\"Background:\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"This project revolves around the valuation of European call options on the S&P 500 index. A European call option\\ngrants the holder the right, but not the obligation, to purchase an asset at a predetermined price, called the strike\\nprice, on a specific expiration date. These options are pivotal financial instruments used for hedging risk, speculative\\ntrading, and enhancing portfolio returns due to their leverage effect.\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Traditionally, the Black-Scholes model has been a foundational tool for pricing European call options. Developed by\\nFischer Black and Myron Scholes in 1973, this model calculates the theoretical value of options using parameters\\nsuch as the strike price, underlying asset price, volatility of the asset, time to expiration, and the risk-free rate of\\nreturn. The modelâ€™s formulation is grounded in the assumption that the asset prices follow a geometric Brownian\\nmotion with constant drift and volatility. In 1997, the importance of the Black-Scholes model was recognized with\\nthe award of the Nobel Prize in Economics.\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"However, despite its widespread use and theoretical appeal, the Black-Scholes model has limitations, particularly in\\nhigh-frequency trading environments where market conditions can change rapidly. It assumes volatility is constant\\nand markets are static without accounting for more complex factors like varying interest rates, transaction costs, or\\ndivergences from the log-normal distribution of asset returns. These simplifications can lead to pricing errors,\\nparticularly for options that are far from expiration or deep in/out of the money.\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Given these limitations, there is a growing interest in exploring alternative models and methodologies that\\nincorporate more dynamic variables and better handle real-world market complexities. Machine learning\\napproaches, for example, offer the potential to model option prices based on non-linear relationships and complex\\nmarket dynamics that traditional models might not capture effectively. This project aims to explore such alternatives,\\ncomparing their performance against the Black-Scholes formula to potentially uncover more robust methods of\\noption valuation.\"}]},{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"datasets\"},\"children\":[{\"type\":\"text\",\"value\":\"Datasets:\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"The dataset focuses on European call option pricing data on the S&P 500, incorporating variables like current asset\\nvalue (S), strike price (K), annual interest rate (r), and time to maturity (tau). For regression analysis, the dependent\\nvariable is the current option value (Value), and for classification analysis, it is the binary indicator (BS), denoting\\nwhether the option's value prediction is over or under the actual value. The dataset includes a training set with\\ndetailed records for each option, and a testing set without labels for Value and BS, intended to validate the\\nperformance of machine learning models developed from the training data.\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"data train\",\"src\":\"/articles/options-pricing-machine-learning/data-train-snapshot.png\"},\"children\":[]},{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"data test\",\"src\":\"/articles/options-pricing-machine-learning/data-test-snapshot.png\"},\"children\":[]},{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"data test 2\",\"src\":\"/articles/options-pricing-machine-learning/data-test-snapshot-2.png\"},\"children\":[]}]},{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"data-preprocessing\"},\"children\":[{\"type\":\"text\",\"value\":\"Data Preprocessing\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"During the data processing phase, multiple steps were undertaken to prepare the data for analysis. These included\\nexploratory data analysis, checking for missing values to ensure data completeness and standardizing data to maintain consistency across the dataset.\"}]},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Missing value check: Upon examination of both the train and test datasets, it was found that there was no\\nmissing data in any of the columns. As a result, there was no need for imputation to fill in missing values.\\n\"},{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"missing value check\",\"src\":\"/articles/options-pricing-machine-learning/missing-value-check.png\"},\"children\":[]}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Data Standardization: Before proceeding with further analysis, data standardization was also carried out.\\nThis step ensured that the dataset had a uniform scale, enhancing the reliability of any analytical methods\\napplied later. Standardizing the data helped in comparing features that had different units or scales\\neffectively.\\n\"},{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"data standardization\",\"src\":\"/articles/options-pricing-machine-learning/data-standardization.png\"},\"children\":[]}]}]},{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"regression-analysis\"},\"children\":[{\"type\":\"text\",\"value\":\"Regression Analysis\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"After preprocessing, the dataset was ready for the data modeling process. To have a robust modeling process,\\n10-fold cross-validation (10-fold CV) was also implemented. Then, initial data modeling was performed for several\\nalgorithms which were Linear Regression, Random Forest, KNN Regression, and Support Vector Regression with\\ndefault hyperparameters used in each algorithm. To test model performance initially, we used the validation set approach by\\nsplitting the available training data into test and train datasets (20% test and 80% train).\"}]},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Linear Regression: Linear regression is a simple yet powerful algorithm that assumes a linear relationship between\\nthe input features and the target variable.\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: The initial modeling using linear regression yielded a mean R-squared score of 92.44% from\\ncross-validation. While linear regression is straightforward and easy to interpret, its performance may be\\nlimited due to potential multicollinearity caused by the correlated variables.\"}]}]}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Random Forest Regression: Random forest regression is an ensemble learning method that builds multiple\\ndecision trees during training and outputs the mean prediction of the individual trees. Its ability to handle non-linear\\nrelationships and feature interactions made it an ideal choice.\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: Random forest regression demonstrated superior performance compared to other algorithms,\\nachieving an impressive mean R-squared score of 99.66%.\"}]}]}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"KNN Regression: K-nearest neighbors (KNN) regression is a non-parametric algorithm that predicts the target\\nvariable by averaging the values of its k-nearest neighbors in the feature space.\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: KNN regression performed reasonably well, yielding a mean R-squared score of 94.82%\\nfrom cross-validation.\"}]}]}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Support Vector Regression (SVR): Support vector regression is a supervised learning algorithm that finds the\\noptimal hyperplane in a high-dimensional space to minimize the error between the predicted and actual values. SVR\\ncan handle non-linear relationships through the use of kernel functions\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: SVR exhibited the lowest performance among the algorithms considered, with a mean\\nR-squared score of 59.15% from cross-validation.\"}]}]}]}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"From the initial modeling result, it can be seen that Random Forest Regression outperformed all other algorithms\\nwith an R-squared score of 99.66%. Then, hyperparameter tuning for each algorithm was performed to fine-tune the\\nmodel as well as to find the best hyperparameter for prediction. The method used for hyperparameter tuning in this\\ncase was GridSearch Cross Validation.\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"10-fold CV regression performances\",\"src\":\"/articles/options-pricing-machine-learning/table-regression-performances.png\"},\"children\":[]}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"From the hyperparameter tuning process, it can be observed that Random Forest Regression still outperformed the\\nother algorithms in terms of R-squared score performance with the optimal hyperparameters making it our final\\nchoice for Value prediction.\"}]},{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"classification-analysis\"},\"children\":[{\"type\":\"text\",\"value\":\"Classification Analysis\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"For predicting the binary status (BS) of over or underestimation in European call option pricing, we evaluated\\nLogistic Regression, Random Forest Classifier, Support Vector Machine (SVM), and K-Nearest Neighbors (KNN)\\nClassifier methods. We tested the performance of these approaches, by comparing their classification errors. To do\\nthis, we used the validation set approach by splitting the available training data into test and train datasets (20% test\\nand 80% train).\"}]},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Logistic Regression: Logistic regression is a linear model used for binary classification tasks, estimating the\\nprobability of a binary outcome based on one or more predictor variables. It was considered due to its\\nstraightforward application in binary classification tasks and its ability to provide probabilities, which are useful for\\nthreshold tuning.\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: Logistic regression achieved a classification error rate of 12.26%.\"}]}]}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Random Forest Classifier: Random forest classifier is an ensemble learning method that constructs multiple\\ndecision trees during training and outputs the mode of the classes (for classification tasks) of the individual trees. Its\\nability to handle complex relationships and reduce overfitting through ensemble learning made it highly suitable for\\nthis task.\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: Random forest classifier demonstrated the best performance, with a classification error rate of\\n6.36%.\"}]}]}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Support Vector Machine (SVM): SVM is a supervised learning algorithm that finds the optimal hyperplane to\\nseparate data points into different classes in a high-dimensional space. It was chosen for its robustness in\\nhigh-dimensional spaces.\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: SVM exhibited a classification error rate of 15.8%.\"}]}]}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"K-Nearest Neighbors (KNN) Classifier: KNN classifier is a simple and interpretable non-parametric algorithm\\nthat assigns a class label to an instance based on the majority class of its k nearest neighbors in the feature space.\\n\"},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Performance: KNN classifier achieved a classification error rate of 14.76%.\\nThis selection process was crucial in ensuring we chose the most suitable model for our classification needs, with\\nthe Random Forest ultimately standing out due to its low error rate and robust performance across various metrics.\"}]}]}]}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"Classification performances\",\"src\":\"/articles/options-pricing-machine-learning/table-classification-performances.png\"},\"children\":[]}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"We see that the Random Forest Error classifier with 200 estimators gave us the lowest classification error of 6.36%\\nmeaning that only 6.36% of options would be misclassified. When it comes to predicting European call option\\npricing, accuracy is the highest priority, leading us to random forest classifier as our final choice for predicting BS\\nover/under prediction. Beyond classification error, Random forest has the following advantages over other models:\"}]},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Robustness to Overfitting and Reduced Variance: By aggregating the predictions of multiple decision\\ntrees, Random Forests are less prone to overfitting and have lower variance.\"}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Feature Importance: By evaluating feature importance based on how much each feature decreases\\nimpurity across all trees, Random Forests provide insights into which features are critical for classification.\\n\"},{\"type\":\"element\",\"tag\":\"img\",\"props\":{\"alt\":\"feature importance\",\"src\":\"/articles/options-pricing-machine-learning/feature-importance.png\"},\"children\":[]}]}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"The classification prediction can further be improved by using ensemble methods that combine multiple Random\\nForest models or other types of models to boost performance further.\"}]},{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"business-impact\"},\"children\":[{\"type\":\"text\",\"value\":\"Business Impact\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Leveraging machine learning models for option pricing compared to the traditional Black-Scholes formula has the\\nfollowing business implications and considerations for real-world application :\"}]},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Including All Predictor Variables: Initially, including all four predictor variables (current asset value,\\nstrike price, interest rate, and time to maturity) can be advantageous. This allows the model to identify\\nwhich factors have the most significant influence on option pricing. Subsequently, feature selection\\ntechniques can be employed to pinpoint the most impactful variables, potentially enhancing model\\nefficiency.\"}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Machine learning vs. Black-Scholes: ML models can outperform the Black-Scholes formula in predicting\\noption values due to their ability to capture complex relationships between multiple variables and the\\noption price that may be neglected otherwise. The Black-Scholes formula relies on assumptions that may\\nnot always reflect real-world market conditions. While machine learning models can achieve higher\\naccuracy by learning complex patterns from historical data, this complexity can lead to models that are\\ndifficult to interpret and computationally expensive to run. Additionally, models trained on historical data\\nmay not generalize well to unseen market conditions.\"}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Model applicability to new stocks: Directly applying the trained model to new stocks is not ideal. The\\nmodel is trained on historical data, and market dynamics can evolve considerably. Before applying the\\nmodel to a specific stock like Tesla, it's advisable to first assess its performance on unseen data from the\\nsame asset class (S&P 500). Additionally, incorporating domain knowledge about the stock market and\\nTesla specifically could further refine the model's accuracy.\"}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Prediction Accuracy vs. Interpretability: In the context of option pricing, prediction accuracy holds\\ngreater significance than interpretability. Options are financial instruments used for managing risk and\\nspeculation. Even minor pricing errors can result in substantial financial losses. While understanding how\\nthe model arrives at a prediction can be valuable (interpretability), it's less critical than the model's ability\\nto deliver a correct option value.\"}]}]},{\"type\":\"element\",\"tag\":\"h3\",\"props\":{\"id\":\"conclusion\"},\"children\":[{\"type\":\"text\",\"value\":\"Conclusion\"}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"The analysis revealed that machine learning models are a viable alternative to the traditional Black-Scholes formula,\\nparticularly when dealing with complex market dynamics. However, there's room for further exploration &\\noptimization:\"}]},{\"type\":\"element\",\"tag\":\"ul\",\"props\":{},\"children\":[{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Segmentation and Specialized Models: Stock options exhibit diverse behaviors based on underlying\\nassets, maturities, and market conditions. Future work could involve segmenting options based on similar\\ncharacteristics and building specialized predictor models for each segment. This approach could potentially\\nimprove overall accuracy by introducing a balance of specificity and generalizability within the segment.\"}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Ensemble Models: Combining the strengths of traditional models, like Black-Scholes, and modern\\nmachine learning models could lead to more accurate predictions.\"}]},{\"type\":\"element\",\"tag\":\"li\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"Incorporating Market Sentiment: Option pricing is influenced not only by fundamental factors but also\\nby market sentiment and broader economic trends. Integrating advanced language models capable of\\nanalyzing news articles, social media data, and other textual sources could enable more holistic prediction\\nmodels that account for these intangible factors.\"}]}]},{\"type\":\"element\",\"tag\":\"p\",\"props\":{},\"children\":[{\"type\":\"text\",\"value\":\"By pursuing these avenues for improvement, we can further refine option pricing models, leading to more informed\\nfinancial decisions and potentially reducing risk in the options market.\"}]}],\"toc\":{\"title\":\"\",\"searchDepth\":2,\"depth\":2,\"links\":[{\"id\":\"authors\",\"depth\":3,\"text\":\"Authors:\"},{\"id\":\"background\",\"depth\":3,\"text\":\"Background:\"},{\"id\":\"datasets\",\"depth\":3,\"text\":\"Datasets:\"},{\"id\":\"data-preprocessing\",\"depth\":3,\"text\":\"Data Preprocessing\"},{\"id\":\"regression-analysis\",\"depth\":3,\"text\":\"Regression Analysis\"},{\"id\":\"classification-analysis\",\"depth\":3,\"text\":\"Classification Analysis\"},{\"id\":\"business-impact\",\"depth\":3,\"text\":\"Business Impact\"},{\"id\":\"conclusion\",\"depth\":3,\"text\":\"Conclusion\"}]}},\"_type\":\"markdown\",\"_id\":\"content:articles:options-pricing-machine-learning.md\",\"_source\":\"content\",\"_file\":\"articles/options-pricing-machine-learning.md\",\"_extension\":\"md\"},\"hash\":\"MbX3H7s79t\"}";

export { optionsPricingMachineLearning as default };
//# sourceMappingURL=options-pricing-machine-learning.mjs.map
